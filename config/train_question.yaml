# Model args
model_name_or_path: /public/home/pengxi_lab/project/LLaVA-ReID/llava-onevision-qwen2-7b-ov # Path to llava-qwen-7b-ov
version: qwen_reid
#mm_tunable_parts: "mm_vision_tower,mm_mlp_adapter,mm_language_model"
mm_tunable_parts: mm_language_model
vision_tower: /public/home/pengxi_lab/project/siglip-so400m-patch14-384
mm_vision_select_layer: -2
mm_use_im_patch_token: false
mm_patch_merge_type: spatial_unpad
image_aspect_ratio: pad
mm_projector_type: mlp2x_gelu
image_grid_pinpoints: null


# Data args
data_path: ./Interactive-PEDES_LLaVA-ReID/training_conversations-c4.json
image_folder: /public/share/pengxi_lab/dataset/text-reid
lazy_preprocess: True

# Training args
lora_enable: true
lora_alpha: 256
lora_r: 128
bits: 4
deepspeed: config/deepspeed/zero2_questioner.json


#mm_selector_lr: 1.0e-4
#mm_projector_lr: 1.0e-5
#mm_vision_tower_lr: 2.0e-6
bf16: True
run_name: questioner-c4
output_dir: ./Interactive-PEDES_LLaVA-ReID
num_train_epochs: 1.0
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
evaluation_strategy: "no"
save_strategy: steps
save_steps: 1000
save_total_limit: 5
learning_rate: 1.0e-5
weight_decay: 0.
warmup_ratio: 0.02
lr_scheduler_type: cosine
logging_steps: 1
tf32: True
model_max_length: 4096
gradient_checkpointing: True
dataloader_num_workers: 1
report_to: none
torch_compile: True
torch_compile_backend: inductor
dataloader_drop_last: True
group_by_modality_length: True
verbose_logging: True
attn_implementation: sdpa
